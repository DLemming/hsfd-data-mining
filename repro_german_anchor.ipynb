{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e32cb1f",
   "metadata": {},
   "source": [
    "# Reproduce the papers results on the German Credit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7431be",
   "metadata": {},
   "source": [
    "### 1. Init & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1804476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set: 700\n",
      "Length test set: 300\n"
     ]
    }
   ],
   "source": [
    "from anchor import anchor_tabular\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "def load_ds(name):\n",
    "    assert name in [\"german\", \"adult\"]\n",
    "\n",
    "    # Load train set\n",
    "    title = \"scripts/datasets/train_set_\"+name+\"_strat.p\"\n",
    "    train = open(title,\"rb\")\n",
    "    train_set: pd.DataFrame = pickle.load(train)\n",
    "\n",
    "    title = \"scripts/datasets/train_label_\"+name+\"_strat.p\"\n",
    "    train_l = open(title,\"rb\")\n",
    "    train_label: pd.DataFrame = pickle.load(train_l)\n",
    "\n",
    "    # Load Test set\n",
    "    title = \"scripts/datasets/test_set_\" + name + \"_strat.p\"\n",
    "    test = open(title, \"rb\")\n",
    "    test_set: pd.DataFrame = pickle.load(test)\n",
    "\n",
    "    title = \"scripts/datasets/test_label_\" + name + \"_strat.p\"\n",
    "    test_l = open(title, \"rb\")\n",
    "    test_label: pd.DataFrame = pickle.load(test_l)\n",
    "\n",
    "    # Reset indices\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    train_label = train_label.reset_index(drop=True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "    test_label = test_label.reset_index(drop=True)\n",
    "\n",
    "    # Convert object columns to int\n",
    "    train_set = train_set.astype(int)\n",
    "    test_set = test_set.astype(int)\n",
    "\n",
    "    return train_set, train_label, test_set, test_label\n",
    "\n",
    "train_set, train_label, test_set, test_label = load_ds(\"german\")\n",
    "\n",
    "print(\"Length train set:\", len(train_set))\n",
    "print(\"Length test set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde023b7",
   "metadata": {},
   "source": [
    "### 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8dfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: str, ds: str):\n",
    "    assert model in [\"catboost\", \"lg\", \"xgb\"]\n",
    "    assert ds in [\"adult\", \"german\"]\n",
    "\n",
    "    path = f\"scripts/results/trained_{model}_{ds}.p\"\n",
    "    path_opened = open(path,\"rb\")\n",
    "\n",
    "    if model == \"lg\":\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model: LogisticRegression = pickle.load(path_opened)\n",
    "    elif model == \"xgb\":\n",
    "        from xgboost import XGBClassifier\n",
    "        model: XGBClassifier = pickle.load(path_opened)\n",
    "    elif model == \"catboost\":\n",
    "        from catboost import CatBoostClassifier\n",
    "        model: CatBoostClassifier = pickle.load(path_opened)\n",
    "\n",
    "    if model is None:\n",
    "        raise Exception()\n",
    "    return model\n",
    "\n",
    "def load_explainer(train_set, train_label):\n",
    "    return anchor_tabular.AnchorTabularExplainer(\n",
    "        class_names=train_label.unique(),\n",
    "        feature_names=train_set.columns,\n",
    "        train_data=train_set.values\n",
    "    )\n",
    "\n",
    "lg = load_model(\"lg\", \"german\")\n",
    "xgb = load_model(\"xgb\", \"german\")\n",
    "cat = load_model(\"catboost\", \"german\")\n",
    "\n",
    "explainer: anchor_tabular.AnchorTabularExplainer = load_explainer(train_set, train_label)\n",
    "\n",
    "def xgb_fn(X):\n",
    "    return xgb.predict(pd.DataFrame(X, columns=test_set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6b5ea",
   "metadata": {},
   "source": [
    "### 3. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47ff151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9233333333333333\n",
      "LogReg Accuracy:  0.7166666666666667\n",
      "CatBoost Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict\n",
    "lg_preds  = lg.predict(test_set)\n",
    "xgb_preds = xgb_fn(test_set)\n",
    "cat_preds = cat.predict(test_set)\n",
    "\n",
    "# Accuracy\n",
    "lg_acc  = accuracy_score(test_label, lg_preds)\n",
    "xgb_acc = accuracy_score(test_label, xgb_preds)\n",
    "cat_acc = accuracy_score(test_label, cat_preds)\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_acc)\n",
    "print(\"LogReg Accuracy: \", lg_acc)\n",
    "print(\"CatBoost Accuracy:\", cat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc8510",
   "metadata": {},
   "source": [
    "### 4a. Calculate Fidelity (seed=1, num_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e0c13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating fidelity. This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fidelity (LG mean=0.763): 100%|██████████| 50/50 [00:08<00:00,  5.59it/s]\n",
      "Fidelity (XGB mean=0.726): 100%|██████████| 50/50 [00:53<00:00,  1.07s/it]\n",
      "Fidelity (CAT mean=0.714): 100%|██████████| 50/50 [00:36<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fidelity results (@0.4-prec, 50 samples):\n",
      "LG:\t0.763\n",
      "XGB:\t0.726\n",
      "CAT:\t0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def conv_rule_to_pandas_query(rule):\n",
    "    \"\"\"\n",
    "    Converts Anchor rules (list of strings) into a valid pandas .query() expression.\n",
    "    Supports compound range rules like '3.00 < occupation <= 5.00'.\n",
    "    \"\"\"\n",
    "    query_parts = []\n",
    "    \n",
    "    for cond in rule:\n",
    "        # Match compound ranges like '3.00 < occupation <= 5.00'\n",
    "        match = re.match(r\"([\\d\\.]+)\\s*<\\s*(.+)\\s*<=\\s*([\\d\\.]+)\", cond)\n",
    "        if match:\n",
    "            low, col, high = match.groups()\n",
    "            col = col.strip()\n",
    "            expr = f\"({float(low)} < `{col}` <= {float(high)})\"\n",
    "            query_parts.append(expr)\n",
    "            continue\n",
    "        \n",
    "        # Match simple binary comparisons\n",
    "        match = re.match(r\"(.+?)\\s*(<=|>=|<|>|==)\\s*([\\d\\.]+)\", cond)\n",
    "        if match:\n",
    "            col, op, val = match.groups()\n",
    "            col = col.strip()\n",
    "            expr = f\"(`{col}` {op} {float(val)})\"\n",
    "            query_parts.append(expr)\n",
    "            continue\n",
    "\n",
    "        raise ValueError(f\"Unsupported condition format: {cond}\")\n",
    "    \n",
    "    return \" & \".join(query_parts)\n",
    "\n",
    "def calculate_fidelity(model_fn, explainer, dataset, tresh=0.95, num_samples=50, name=None):\n",
    "    fidelity_scores = []\n",
    "    sampled_count = 0\n",
    "\n",
    "    # Randomly shuffle indices, see one instance at most once\n",
    "    random_indices = np.random.permutation(len(dataset))\n",
    "\n",
    "    with tqdm(total=num_samples, desc=\"Fidelity\") as pbar:\n",
    "        while len(fidelity_scores) < num_samples:  # Continue until we have num_samples valid instances explained\n",
    "            # Get a new instance and its true label\n",
    "            \n",
    "            # numpy random approaches\n",
    "            i = random_indices[sampled_count]                    # LG: 0.763, XGB: 0.726, CAT: 0.714\n",
    "            # i = np.random.choice(len(dataset), replace=False)  # LG: 0.754, XGB: 0.714, CAT: 0.752\n",
    "            # i = np.random.choice(len(dataset), replace=True)   # LG: 0.768, XGB: 0.731, CAT: 0.714\n",
    "\n",
    "            # default random\n",
    "            # i = random.randrange(len(dataset))                 # LG: 0.759, XGB: 0.727, CAT: 0.702\n",
    "\n",
    "            x = dataset.iloc[[i]]\n",
    "            y_true = model_fn(x).item()  # model prediction for the instance\n",
    "            sampled_count += 1\n",
    "\n",
    "            # Explain the instance with Anchor\n",
    "            exp = explainer.explain_instance(x.values, classifier_fn=model_fn, threshold=tresh)\n",
    "            rule = exp.names()  # e.g., [\"age > 30\", \"capital-gain <= 0\"]\n",
    "\n",
    "            if not rule:\n",
    "                continue  # Skip if no rule found\n",
    "\n",
    "            # Convert the Anchor rule to a pandas query expression\n",
    "            query_str = conv_rule_to_pandas_query(rule)\n",
    "\n",
    "            # Check how many of the covered instances match the model's original prediction\n",
    "            covered = dataset.query(query_str)\n",
    "            pred_match = model_fn(covered) == y_true\n",
    "            fidelity = pred_match.mean()\n",
    "            fidelity_scores.append(fidelity)\n",
    "\n",
    "            # Update pbar and running mean\n",
    "            running_mean = np.mean(fidelity_scores)\n",
    "            pbar.set_description(f\"Fidelity ({name} mean={running_mean:.3f})\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    return np.mean(fidelity_scores)\n",
    "\n",
    "\n",
    "thresh = 0.4 # anchor design: 0.95\n",
    "num_samples = 50\n",
    "model_fns = [lg.predict, xgb_fn, cat.predict]\n",
    "model_names = [\"LG\", \"XGB\", \"CAT\"]\n",
    "results = {}\n",
    "\n",
    "print(\"Calculating fidelity. This might take a while...\")\n",
    "for fn, name in zip(model_fns, model_names):\n",
    "    results[name] = calculate_fidelity(fn, explainer, test_set, tresh=thresh, num_samples=num_samples, name=name)\n",
    "\n",
    "print(f\"\\nFidelity results (@{thresh}-prec, {num_samples} samples):\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\t{value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd59c4",
   "metadata": {},
   "source": [
    "### 4b. Average fidelity over different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45263300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fidelity (LG (seed=0) mean=0.753): 100%|██████████| 50/50 [00:09<00:00,  5.29it/s]\n",
      "Fidelity (XGB (seed=0) mean=0.692): 100%|██████████| 50/50 [00:47<00:00,  1.06it/s]\n",
      "Fidelity (CAT (seed=0) mean=0.722): 100%|██████████| 50/50 [00:36<00:00,  1.38it/s]\n",
      "Fidelity (LG (seed=1) mean=0.763): 100%|██████████| 50/50 [00:08<00:00,  5.67it/s]\n",
      "Fidelity (XGB (seed=1) mean=0.726): 100%|██████████| 50/50 [00:43<00:00,  1.15it/s]\n",
      "Fidelity (CAT (seed=1) mean=0.714): 100%|██████████| 50/50 [00:35<00:00,  1.40it/s]\n",
      "Fidelity (LG (seed=2) mean=0.757): 100%|██████████| 50/50 [00:09<00:00,  5.47it/s]\n",
      "Fidelity (XGB (seed=2) mean=0.710): 100%|██████████| 50/50 [00:48<00:00,  1.04it/s]\n",
      "Fidelity (CAT (seed=2) mean=0.704): 100%|██████████| 50/50 [00:36<00:00,  1.36it/s]\n",
      "Fidelity (LG (seed=3) mean=0.740): 100%|██████████| 50/50 [00:08<00:00,  5.62it/s]\n",
      "Fidelity (XGB (seed=3) mean=0.738): 100%|██████████| 50/50 [00:51<00:00,  1.04s/it]\n",
      "Fidelity (CAT (seed=3) mean=0.726): 100%|██████████| 50/50 [00:36<00:00,  1.38it/s]\n",
      "Fidelity (LG (seed=4) mean=0.759): 100%|██████████| 50/50 [00:08<00:00,  5.60it/s]\n",
      "Fidelity (XGB (seed=4) mean=0.692): 100%|██████████| 50/50 [00:54<00:00,  1.10s/it]\n",
      "Fidelity (CAT (seed=4) mean=0.758): 100%|██████████| 50/50 [00:31<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fidelity results (@0.4-prec, 50 samples) across 5 seeds:\n",
      "LG:\tmean=0.754 | std=0.008 | scores=[0.7533726470333711, 0.7625488683833689, 0.7570759809550154, 0.7402518396113595, 0.7591722260934225]\n",
      "XGB:\tmean=0.711 | std=0.018 | scores=[0.6922379203022284, 0.7258650203118172, 0.7095291670364087, 0.7376676869178961, 0.6921323060813005]\n",
      "CAT:\tmean=0.725 | std=0.018 | scores=[0.7221060115429891, 0.7138089087718369, 0.7041785441246404, 0.7262858023083861, 0.7576935245797166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.4 # anchor design: 0.95\n",
    "num_samples = 50\n",
    "different_seeds = 5\n",
    "\n",
    "model_fns = [lg.predict, xgb_fn, cat.predict]\n",
    "model_names = [\"LG\", \"XGB\", \"CAT\"]\n",
    "results = {name: [] for name in model_names}\n",
    "\n",
    "for seed in np.arange(different_seeds):\n",
    "    np.random.seed(seed)\n",
    "    for fn, name in zip(model_fns, model_names):\n",
    "        score = calculate_fidelity(fn, explainer, test_set, tresh=thresh, num_samples=num_samples, name=f\"{name} (seed={seed})\")\n",
    "        results[name].append(score)\n",
    "\n",
    "print(f\"\\nFidelity results (@{thresh}-prec, {num_samples} samples) across {different_seeds} seeds:\")\n",
    "for key, scores in results.items():\n",
    "    mean_val = np.mean(scores)\n",
    "    std_val = np.std(scores)\n",
    "    print(f\"{key}:\\tmean={mean_val:.3f} | std={std_val:.3f} | scores={scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6e52e",
   "metadata": {},
   "source": [
    "### 5. Calculate Stability (naive approach, only intuitive stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b87884aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stability...: 100%|██████████| 5/5 [00:08<00:00,  1.65s/it]\n",
      "Calculating stability...: 100%|██████████| 5/5 [00:00<00:00, 5137.56it/s]\n",
      "Calculating stability...: 100%|██████████| 5/5 [00:25<00:00,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG Stability: 1.0323140671941176\n",
      "XGB Stability: 0.0\n",
      "CAT Stability: 0.896661287868017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def jaccard_distance(exp_x: set, exp_x_neigh: set, debug):\n",
    "    \"\"\"Distance between two explanations.\"\"\"\n",
    "    union = exp_x | exp_x_neigh\n",
    "    if not union:\n",
    "        return 0.0  # If both are empty, treat as identical\n",
    "    jacc = 1 - len(exp_x & exp_x_neigh) / len(union)\n",
    "    if debug:\n",
    "        print(\"\\tJacc:\\t\", jacc)\n",
    "    return jacc\n",
    "\n",
    "def hamming_distance(exp_x: set, exp_x_neigh: set, debug=False):\n",
    "    \"\"\"Distance between two explanations using Hamming distance.\"\"\"\n",
    "    # Calculate the symmetric difference between the two sets\n",
    "    diff = exp_x ^ exp_x_neigh  # ^ is symmetric difference\n",
    "    # Normalize the distance by the maximum possible number of differences (size of the union)\n",
    "    max_diff = max(len(exp_x), len(exp_x_neigh))\n",
    "    if max_diff == 0:\n",
    "        return 0.0  # If both are empty, treat as identical\n",
    "    \n",
    "    hamming = len(diff) / max_diff\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\tHamming:\\t\", hamming)\n",
    "    \n",
    "    return hamming\n",
    "\n",
    "def eucl_distance(x1: np.ndarray, x2: np.ndarray, debug):\n",
    "    \"\"\"Euclidean distance between inputs.\"\"\"\n",
    "    eucl = np.linalg.norm(x1 - x2)\n",
    "    if debug:\n",
    "        print(\"\\tEucl:\\t\", eucl)\n",
    "    return eucl\n",
    "\n",
    "def norm_eucl_distance(x1: np.ndarray, x2: np.ndarray, scaler: MinMaxScaler, debug):\n",
    "    \"\"\"Euclidean distance between inputs, first normalizing features.\"\"\"\n",
    "    \n",
    "    # Normalize the input vectors (x1 and x2)\n",
    "    x1_normalized = scaler.transform([x1])\n",
    "    x2_normalized = scaler.transform([x2])\n",
    "\n",
    "    # Calculate the Euclidean distance between the normalized vectors\n",
    "    eucl_dist = np.linalg.norm(x1_normalized - x2_normalized)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\tEuc_norm:\\t\", eucl_dist)\n",
    "        \n",
    "    return eucl_dist\n",
    "\n",
    "def calc_lipschitz(exp_x, exp_xp, x, xp, debug, sim, scaler=None):\n",
    "    if sim == \"jaccard\":\n",
    "        exp_dist = jaccard_distance(exp_x, exp_xp, debug)\n",
    "    else:\n",
    "        exp_dist = hamming_distance(exp_x, exp_xp, debug)\n",
    "\n",
    "    if scaler is not None:\n",
    "        input_dist = norm_eucl_distance(x, xp, scaler, debug)\n",
    "    else:\n",
    "        input_dist = eucl_distance(x, xp, debug)\n",
    "\n",
    "    lip = exp_dist/input_dist\n",
    "    if debug:\n",
    "        print(\"\\tLip:\\t\", lip)\n",
    "    return lip\n",
    "\n",
    "def generate_perturbation_neighborhood(x: pd.Series, dataset: pd.DataFrame, num_samples=10, max_perturbation=2):\n",
    "    \"\"\"Generates a synthetic neighborhood around x by applying small perturbations.\"\"\"\n",
    "    neighborhood = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        x_prime = deepcopy(x)\n",
    "\n",
    "        for col in dataset.columns:\n",
    "            # Apply a small perturbation, limiting to max_perturbation in magnitude\n",
    "            perturbation = np.random.randint(-max_perturbation, max_perturbation + 1)\n",
    "            new_value = x[col] + perturbation\n",
    "\n",
    "            # Ensure the perturbed value is non-negative (not below 0)\n",
    "            x_prime[col] = max(0, new_value)\n",
    "\n",
    "        neighborhood.append(x_prime.values)\n",
    "\n",
    "    return np.array(neighborhood)\n",
    "\n",
    "def sample_neighborhood(x: np.ndarray, nn: NearestNeighbors, dataset: pd.DataFrame, k=10):\n",
    "    _, indices = nn.kneighbors(x, n_neighbors=k)\n",
    "    neigh_idcs = indices[0][1:]\n",
    "    neigh_vals = dataset.iloc[neigh_idcs].values\n",
    "    return neigh_vals\n",
    "\n",
    "\n",
    "def calculate_stability(model, explainer, dataset, k_neighbors=10, thresh=0.95, num_samples=1, debug=False, neigh=\"gen\", sim=\"jaccard\", norm=False):\n",
    "    assert neigh in [\"gen\",\"sampled\"]\n",
    "    assert sim in [\"jaccard\",\"hamming\"]\n",
    "\n",
    "    stability_scores = []\n",
    "    if neigh == \"sampled\":\n",
    "        nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(dataset.values)\n",
    "\n",
    "    if norm:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit_transform(dataset.values)\n",
    "\n",
    "    for i in tqdm(range(num_samples), desc=\"Calculating stability...\"):\n",
    "        x = dataset.iloc[i]\n",
    "        x_val = x.values.reshape(1, -1)\n",
    "        if debug:\n",
    "            print(\"Row:\\t\", list(x.values))\n",
    "\n",
    "        try:\n",
    "            anchor_x = explainer.explain_instance(x_val[0], classifier_fn=model.predict, threshold=thresh)\n",
    "            rule_x = set(anchor_x.names())\n",
    "            if not rule_x:\n",
    "                if debug:\n",
    "                    print(f\"No rule on index {i}. Skipping instance...\")\n",
    "                continue  # Skip if no rule was found\n",
    "            if debug:\n",
    "                print(\"Exp:\\t\", anchor_x.names())\n",
    "        except:\n",
    "            if debug:\n",
    "                print(f\"Anchor failed on index {i}. Skipping instance...\")\n",
    "            continue  # Skip instances where anchor fails\n",
    "\n",
    "        # Find neighbors (excluding self)\n",
    "        if neigh == \"gen\":\n",
    "            neigh_vals = generate_perturbation_neighborhood(x, dataset, num_samples=k_neighbors)\n",
    "        else:\n",
    "            neigh_vals = sample_neighborhood(x_val, nn, dataset)\n",
    "\n",
    "        lipschitz_vals = []\n",
    "        for j, x_prime in enumerate(neigh_vals):\n",
    "            if debug:\n",
    "                print(f\"\\n\\tNN{j}:\\t\", list(x_prime))\n",
    "            #try:\n",
    "            anchor_xp = explainer.explain_instance(x_prime.reshape(1, -1), classifier_fn=model.predict, threshold=thresh)\n",
    "            if debug:\n",
    "                print(\"\\tExp:\\t\", anchor_xp.names())\n",
    "            \n",
    "            rule_xp = set(anchor_xp.names())\n",
    "\n",
    "            lip = calc_lipschitz(\n",
    "                exp_x=rule_x,\n",
    "                exp_xp=rule_xp,\n",
    "                x=x.values,\n",
    "                xp=x_prime,\n",
    "                debug=debug,\n",
    "                sim=sim,\n",
    "                scaler=scaler if norm else None\n",
    "            )\n",
    "            lipschitz_vals.append(lip)\n",
    "            #except:\n",
    "                #print(f\"Error occured in neighbor explanation. Skipping instance.\")\n",
    "                #continue  # If explanation fails, skip\n",
    "\n",
    "        if lipschitz_vals:\n",
    "            max_lip = np.max(lipschitz_vals)\n",
    "            if debug:\n",
    "                print(\"\\nMax Lipstein:\\t\", max_lip)\n",
    "            stability_scores.append(max_lip)\n",
    "\n",
    "    return np.mean(stability_scores) if stability_scores else 0.0\n",
    "\n",
    "num_samples = 5\n",
    "debug = False\n",
    "neigh = \"sampled\" # \"sampled\" | \"gen\"\n",
    "sim = \"hamming\" # \"hamming\" | \"jaccard\"\n",
    "norm = True\n",
    "\n",
    "stabilities = []\n",
    "for model in [lg, xgb, cat]:\n",
    "    stability = calculate_stability(\n",
    "        model, explainer, test_set, \n",
    "        num_samples=num_samples,\n",
    "        debug=debug,\n",
    "        neigh=neigh,\n",
    "        sim=sim,\n",
    "        norm=norm\n",
    "    )\n",
    "    stabilities.append(stability)\n",
    "\n",
    "print(\"LG Stability:\", stabilities[0])\n",
    "print(\"XGB Stability:\", stabilities[1])\n",
    "print(\"CAT Stability:\", stabilities[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c3777",
   "metadata": {},
   "source": [
    "## Brute Force Paper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e393eac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Preprocessing: l1 ==\n",
      "  -- Distance: manhattan\n",
      "    >> Model: lg\n",
      "      || Exp-diff: jaccard\n",
      "      Lip: 166.87254345345002\n",
      "      Std: 91.36923906920532\n",
      "      Err: 65.80254345345003\n",
      "    >> Model: xgb\n",
      "      || Exp-diff: jaccard\n",
      "      Lip: 165.34293942604475\n",
      "      Std: 101.24029409567625\n",
      "      Err: 43.942939426044745\n",
      "    >> Model: cat\n",
      "      || Exp-diff: jaccard\n",
      "      Lip: 137.6822337527732\n",
      "      Std: 88.34920868884915\n",
      "      Err: 13.89223375277318\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    Normalizer,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    QuantileTransformer,\n",
    ")\n",
    "\n",
    "\n",
    "preprocessing = {\n",
    "    #\"raw\": lambda x: x,\n",
    "    \"l1\": Normalizer(norm='l1'),\n",
    "    #\"l2\": Normalizer(norm='l2'),\n",
    "    #\"zscore\": StandardScaler(),\n",
    "    #\"minmax\": MinMaxScaler(),\n",
    "    #\"robust\": RobustScaler(),\n",
    "    #\"quantile_normal\": QuantileTransformer(output_distribution='normal'),\n",
    "    #\"quantile_uniform\": QuantileTransformer(output_distribution='uniform'),\n",
    "}\n",
    "distance_metrics = {\n",
    "    #\"euclidean\": lambda x1, x2: np.linalg.norm(x1 - x2, ord=2),\n",
    "    \"manhattan\": lambda x1, x2: np.linalg.norm(x1 - x2, ord=1),\n",
    "    #\"cosine\": lambda x1, x2: distance.cosine(x1, x2),\n",
    "}\n",
    "expl_differences = {\n",
    "    \"jaccard\": lambda exp1, exp2: jaccard_distance(exp1, exp2),\n",
    "    #\"hamming\": lambda exp1, exp2: hamming_distance(exp1, exp2),\n",
    "}\n",
    "model_fns = {\n",
    "    \"lg\": lg.predict,\n",
    "    \"xgb\": xgb_fn,\n",
    "    \"cat\": cat.predict,\n",
    "}\n",
    "paper = {\n",
    "    \"means\": {\n",
    "        \"lg\": 101.07,\n",
    "        \"xgb\": 121.40,\n",
    "        \"cat\": 123.79,\n",
    "    },\n",
    "    \"std\": {\n",
    "        \"lg\": 62.75,\n",
    "        \"xgb\": 98.43,\n",
    "        \"cat\": 76.86,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "num_samples = 30\n",
    "k_neighbors = 10\n",
    "thresh = 0.4\n",
    "\n",
    "\n",
    "def jaccard_distance(exp_x: set, exp_x_neigh: set, debug=False):\n",
    "    \"\"\"Distance between two explanations.\"\"\"\n",
    "    union = exp_x | exp_x_neigh\n",
    "    if not union:\n",
    "        return 0.0  # If both are empty, treat as identical\n",
    "    jacc = 1 - len(exp_x & exp_x_neigh) / len(union)\n",
    "    if debug:\n",
    "        print(\"\\tJacc:\\t\", jacc)\n",
    "    return jacc\n",
    "\n",
    "def hamming_distance(exp_x: set, exp_x_neigh: set, debug=False):\n",
    "    \"\"\"Distance between two explanations using Hamming distance.\"\"\"\n",
    "    # Calculate the symmetric difference between the two sets\n",
    "    diff = exp_x ^ exp_x_neigh  # ^ is symmetric difference\n",
    "    # Normalize the distance by the maximum possible number of differences (size of the union)\n",
    "    max_diff = max(len(exp_x), len(exp_x_neigh))\n",
    "    if max_diff == 0:\n",
    "        return 0.0  # If both are empty, treat as identical\n",
    "    \n",
    "    hamming = len(diff) / max_diff\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\tHamming:\\t\", hamming)\n",
    "    \n",
    "    return hamming\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(test_set.values)\n",
    "def neighbors_of(x: np.ndarray):\n",
    "    _, indices = nn.kneighbors(x, n_neighbors=k_neighbors)\n",
    "    neigh_idcs = indices[0][1:]\n",
    "    neigh_vals = test_set.iloc[neigh_idcs].values\n",
    "    return neigh_vals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sample_indices = np.random.choice(len(test_set), size=num_samples, replace=False)\n",
    "# Randomly shuffle indices, see one instance at most once\n",
    "random_indices = np.random.permutation(len(test_set))\n",
    "stability_results = []\n",
    "\n",
    "for pre_name, pre in preprocessing.items():\n",
    "    print(f\"== Preprocessing: {pre_name} ==\")\n",
    "\n",
    "    # Fit-transform\n",
    "    if callable(pre):  # for \"raw\"\n",
    "        X_prep = pre(test_set.values)\n",
    "    else:\n",
    "        X_prep = pre.fit_transform(test_set.values)\n",
    "\n",
    "    for dist_name, dist_fn in distance_metrics.items():\n",
    "        print(f\"  -- Distance: {dist_name}\")\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=k_neighbors + 1, metric=\"euclidean\")\n",
    "        nn.fit(X_prep)\n",
    "\n",
    "        for model_name, model_fn in model_fns.items():\n",
    "            print(f\"    >> Model: {model_name}\")\n",
    "\n",
    "            for expl_name, expl_diff_fn in expl_differences.items():\n",
    "                print(f\"      || Exp-diff: {expl_name}\")\n",
    "\n",
    "                L_x = []\n",
    "                for i in random_indices:\n",
    "                    if len(L_x) >= num_samples:\n",
    "                        break\n",
    "\n",
    "                    x = test_set.values[i]\n",
    "                    x_prep = X_prep[i].reshape(1, -1)\n",
    "                    _, indices = nn.kneighbors(x_prep)\n",
    "                    neigh_idcs = indices[0][1:]\n",
    "                    \n",
    "                    x_anchor = explainer.explain_instance(\n",
    "                        x, classifier_fn=model_fn, threshold=thresh\n",
    "                    )\n",
    "                    rule_x = set(x_anchor.names())\n",
    "                    if not rule_x:\n",
    "                        continue\n",
    "\n",
    "                    lips = []\n",
    "                    for j in neigh_idcs:\n",
    "                        x_prime = test_set.values[j]\n",
    "                        x_prime_prep = X_prep[j]\n",
    "\n",
    "                        x_prime_anchor = explainer.explain_instance(\n",
    "                            x_prime, classifier_fn=model_fn, threshold=thresh\n",
    "                        )\n",
    "                        rule_xp = set(x_prime_anchor.names())\n",
    "                        if not rule_xp:\n",
    "                            continue\n",
    "\n",
    "                        # Explanation distance\n",
    "                        expl_diff = expl_diff_fn(rule_x, rule_xp)\n",
    "\n",
    "                        # Input distance\n",
    "                        input_dist = dist_fn(X_prep[i], x_prime_prep)\n",
    "                        if input_dist == 0:\n",
    "                            print(\"Input distance == 0, skipping...\")\n",
    "                            continue  # skip duplicates or same point\n",
    "\n",
    "                        lip = expl_diff / input_dist\n",
    "                        lips.append(lip)\n",
    "                    \n",
    "                    if not lips:\n",
    "                        continue\n",
    "\n",
    "                    L_x.append(max(lips))\n",
    "\n",
    "                lip_mean = np.mean(L_x)\n",
    "                lip_std = np.std(L_x)\n",
    "\n",
    "                stability_results.append({\n",
    "                    \"preprocessing\": pre_name,\n",
    "                    \"distance\": dist_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"lipschitz_vals\": L_x,\n",
    "                    \"lipschitz_mean\": lip_mean,\n",
    "                    \"lipschitz_std\": lip_std\n",
    "                })\n",
    "                print(\"      Lip:\", lip_mean)\n",
    "                print(\"      Std:\", lip_std)\n",
    "                print(\"      Err:\", abs(np.mean(L_x) - paper[\"means\"][model_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-survey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
